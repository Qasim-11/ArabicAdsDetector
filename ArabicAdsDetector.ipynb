{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import regex\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import os\n",
        "# import copy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_gRvSgH7_sk",
        "outputId": "51b922b2-a925-465a-d631-c2dfdd28fc8a"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv(\"positive.csv\")\n",
        "df2 = pd.read_csv(\"negative.csv\")\n",
        "df = pd.concat([df1, df2], ignore_index=True)\n"
      ],
      "metadata": {
        "id": "IvKNJF1X8P35"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "5dj4NhN57soe"
      },
      "outputs": [],
      "source": [
        "\n",
        "def separate_emojis(words):\n",
        "    separated = []\n",
        "    emoji_pattern = regex.compile(r'\\p{Emoji}', flags=regex.UNICODE)\n",
        "\n",
        "    for word in words:\n",
        "        current = \"\"\n",
        "        for char in word:\n",
        "            if char.isdigit():\n",
        "                current += char\n",
        "            elif emoji_pattern.match(char):\n",
        "                print(f\"Separating emoji: {char}\")\n",
        "                if current:\n",
        "                    separated.append(current)\n",
        "                    current = \"\"\n",
        "                separated.append(char)\n",
        "            else:\n",
        "                current += char\n",
        "        if current:\n",
        "            separated.append(current)\n",
        "\n",
        "    return separated\n",
        "\n",
        "def is_arabic(word):\n",
        "    return any('\\u0600' <= char <= '\\u06FF' for char in word)\n",
        "def is_english(word):\n",
        "    return all('a' <= char.lower() <= 'z' for char in word)\n",
        "def is_emoji(word):\n",
        "    emoji_pattern = regex.compile(r'\\p{Emoji}', flags=regex.UNICODE)\n",
        "    return any(emoji_pattern.match(char) for char in word)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv02\")\n",
        "embedder = AutoModel.from_pretrained(\"aubmindlab/bert-base-arabertv02\")"
      ],
      "metadata": {
        "id": "sZq7l5vO7-Dm"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['embedding'] = None\n",
        "\n",
        "for i in range(len(df)):\n",
        "  text = df.iloc[i][\"text\"]\n",
        "  tokens = tokenizer(text, return_tensors=\"pt\")\n",
        "  embedding = embedder(**tokens) # equivilant to embedder(tokens['input_ids'],tokens['attention_mask'])\n",
        "  embedding = embedding[0][0].detach().cpu().numpy()\n",
        "\n",
        "  df.at[i, 'embedding'] = embedding\n",
        "\n",
        "  print(f\"{i} Element has beed embedded with shape {embedding.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UygeQleCEg6s",
        "outputId": "d1423f78-9e77-4db3-81d9-e18257fd4ef1"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Element has beed embedded with shape (115, 768)\n",
            "1 Element has beed embedded with shape (163, 768)\n",
            "2 Element has beed embedded with shape (82, 768)\n",
            "3 Element has beed embedded with shape (30, 768)\n",
            "4 Element has beed embedded with shape (102, 768)\n",
            "5 Element has beed embedded with shape (240, 768)\n",
            "6 Element has beed embedded with shape (76, 768)\n",
            "7 Element has beed embedded with shape (51, 768)\n",
            "8 Element has beed embedded with shape (83, 768)\n",
            "9 Element has beed embedded with shape (23, 768)\n",
            "10 Element has beed embedded with shape (88, 768)\n",
            "11 Element has beed embedded with shape (120, 768)\n",
            "12 Element has beed embedded with shape (97, 768)\n",
            "13 Element has beed embedded with shape (120, 768)\n",
            "14 Element has beed embedded with shape (68, 768)\n",
            "15 Element has beed embedded with shape (15, 768)\n",
            "16 Element has beed embedded with shape (19, 768)\n",
            "17 Element has beed embedded with shape (74, 768)\n",
            "18 Element has beed embedded with shape (19, 768)\n",
            "19 Element has beed embedded with shape (73, 768)\n",
            "20 Element has beed embedded with shape (98, 768)\n",
            "21 Element has beed embedded with shape (24, 768)\n",
            "22 Element has beed embedded with shape (83, 768)\n",
            "23 Element has beed embedded with shape (95, 768)\n",
            "24 Element has beed embedded with shape (79, 768)\n",
            "25 Element has beed embedded with shape (69, 768)\n",
            "26 Element has beed embedded with shape (30, 768)\n",
            "27 Element has beed embedded with shape (19, 768)\n",
            "28 Element has beed embedded with shape (33, 768)\n",
            "29 Element has beed embedded with shape (99, 768)\n",
            "30 Element has beed embedded with shape (25, 768)\n",
            "31 Element has beed embedded with shape (79, 768)\n",
            "32 Element has beed embedded with shape (21, 768)\n",
            "33 Element has beed embedded with shape (60, 768)\n",
            "34 Element has beed embedded with shape (32, 768)\n",
            "35 Element has beed embedded with shape (112, 768)\n",
            "36 Element has beed embedded with shape (68, 768)\n",
            "37 Element has beed embedded with shape (63, 768)\n",
            "38 Element has beed embedded with shape (61, 768)\n",
            "39 Element has beed embedded with shape (42, 768)\n",
            "40 Element has beed embedded with shape (55, 768)\n",
            "41 Element has beed embedded with shape (43, 768)\n",
            "42 Element has beed embedded with shape (59, 768)\n",
            "43 Element has beed embedded with shape (22, 768)\n",
            "44 Element has beed embedded with shape (40, 768)\n",
            "45 Element has beed embedded with shape (124, 768)\n",
            "46 Element has beed embedded with shape (63, 768)\n",
            "47 Element has beed embedded with shape (42, 768)\n",
            "48 Element has beed embedded with shape (9, 768)\n",
            "49 Element has beed embedded with shape (8, 768)\n",
            "50 Element has beed embedded with shape (8, 768)\n",
            "51 Element has beed embedded with shape (9, 768)\n",
            "52 Element has beed embedded with shape (7, 768)\n",
            "53 Element has beed embedded with shape (6, 768)\n",
            "54 Element has beed embedded with shape (8, 768)\n",
            "55 Element has beed embedded with shape (11, 768)\n",
            "56 Element has beed embedded with shape (7, 768)\n",
            "57 Element has beed embedded with shape (8, 768)\n",
            "58 Element has beed embedded with shape (7, 768)\n",
            "59 Element has beed embedded with shape (7, 768)\n",
            "60 Element has beed embedded with shape (7, 768)\n",
            "61 Element has beed embedded with shape (10, 768)\n",
            "62 Element has beed embedded with shape (9, 768)\n",
            "63 Element has beed embedded with shape (7, 768)\n",
            "64 Element has beed embedded with shape (10, 768)\n",
            "65 Element has beed embedded with shape (7, 768)\n",
            "66 Element has beed embedded with shape (6, 768)\n",
            "67 Element has beed embedded with shape (5, 768)\n",
            "68 Element has beed embedded with shape (9, 768)\n",
            "69 Element has beed embedded with shape (39, 768)\n",
            "70 Element has beed embedded with shape (22, 768)\n",
            "71 Element has beed embedded with shape (8, 768)\n",
            "72 Element has beed embedded with shape (26, 768)\n",
            "73 Element has beed embedded with shape (17, 768)\n",
            "74 Element has beed embedded with shape (11, 768)\n",
            "75 Element has beed embedded with shape (50, 768)\n",
            "76 Element has beed embedded with shape (22, 768)\n",
            "77 Element has beed embedded with shape (18, 768)\n",
            "78 Element has beed embedded with shape (34, 768)\n",
            "79 Element has beed embedded with shape (14, 768)\n",
            "80 Element has beed embedded with shape (15, 768)\n",
            "81 Element has beed embedded with shape (28, 768)\n",
            "82 Element has beed embedded with shape (15, 768)\n",
            "83 Element has beed embedded with shape (16, 768)\n",
            "84 Element has beed embedded with shape (16, 768)\n",
            "85 Element has beed embedded with shape (10, 768)\n",
            "86 Element has beed embedded with shape (17, 768)\n",
            "87 Element has beed embedded with shape (21, 768)\n",
            "88 Element has beed embedded with shape (11, 768)\n",
            "89 Element has beed embedded with shape (25, 768)\n",
            "90 Element has beed embedded with shape (26, 768)\n",
            "91 Element has beed embedded with shape (8, 768)\n",
            "92 Element has beed embedded with shape (42, 768)\n",
            "93 Element has beed embedded with shape (12, 768)\n",
            "94 Element has beed embedded with shape (25, 768)\n",
            "95 Element has beed embedded with shape (8, 768)\n",
            "96 Element has beed embedded with shape (7, 768)\n",
            "97 Element has beed embedded with shape (9, 768)\n",
            "98 Element has beed embedded with shape (5, 768)\n",
            "99 Element has beed embedded with shape (5, 768)\n",
            "100 Element has beed embedded with shape (19, 768)\n",
            "101 Element has beed embedded with shape (41, 768)\n",
            "102 Element has beed embedded with shape (9, 768)\n",
            "103 Element has beed embedded with shape (5, 768)\n",
            "104 Element has beed embedded with shape (12, 768)\n",
            "105 Element has beed embedded with shape (10, 768)\n",
            "106 Element has beed embedded with shape (28, 768)\n",
            "107 Element has beed embedded with shape (12, 768)\n",
            "108 Element has beed embedded with shape (9, 768)\n",
            "109 Element has beed embedded with shape (14, 768)\n",
            "110 Element has beed embedded with shape (11, 768)\n",
            "111 Element has beed embedded with shape (6, 768)\n",
            "112 Element has beed embedded with shape (12, 768)\n",
            "113 Element has beed embedded with shape (8, 768)\n",
            "114 Element has beed embedded with shape (5, 768)\n",
            "115 Element has beed embedded with shape (21, 768)\n",
            "116 Element has beed embedded with shape (9, 768)\n",
            "117 Element has beed embedded with shape (61, 768)\n",
            "118 Element has beed embedded with shape (27, 768)\n",
            "119 Element has beed embedded with shape (9, 768)\n",
            "120 Element has beed embedded with shape (14, 768)\n",
            "121 Element has beed embedded with shape (11, 768)\n",
            "122 Element has beed embedded with shape (9, 768)\n",
            "123 Element has beed embedded with shape (12, 768)\n",
            "124 Element has beed embedded with shape (23, 768)\n",
            "125 Element has beed embedded with shape (11, 768)\n",
            "126 Element has beed embedded with shape (52, 768)\n",
            "127 Element has beed embedded with shape (9, 768)\n",
            "128 Element has beed embedded with shape (12, 768)\n",
            "129 Element has beed embedded with shape (10, 768)\n",
            "130 Element has beed embedded with shape (28, 768)\n",
            "131 Element has beed embedded with shape (15, 768)\n",
            "132 Element has beed embedded with shape (16, 768)\n",
            "133 Element has beed embedded with shape (6, 768)\n",
            "134 Element has beed embedded with shape (8, 768)\n",
            "135 Element has beed embedded with shape (18, 768)\n",
            "136 Element has beed embedded with shape (28, 768)\n",
            "137 Element has beed embedded with shape (18, 768)\n",
            "138 Element has beed embedded with shape (30, 768)\n",
            "139 Element has beed embedded with shape (8, 768)\n",
            "140 Element has beed embedded with shape (41, 768)\n",
            "141 Element has beed embedded with shape (11, 768)\n",
            "142 Element has beed embedded with shape (9, 768)\n",
            "143 Element has beed embedded with shape (15, 768)\n",
            "144 Element has beed embedded with shape (34, 768)\n",
            "145 Element has beed embedded with shape (7, 768)\n",
            "146 Element has beed embedded with shape (10, 768)\n",
            "147 Element has beed embedded with shape (6, 768)\n",
            "148 Element has beed embedded with shape (13, 768)\n",
            "149 Element has beed embedded with shape (6, 768)\n",
            "150 Element has beed embedded with shape (5, 768)\n",
            "151 Element has beed embedded with shape (12, 768)\n",
            "152 Element has beed embedded with shape (31, 768)\n",
            "153 Element has beed embedded with shape (14, 768)\n",
            "154 Element has beed embedded with shape (13, 768)\n",
            "155 Element has beed embedded with shape (78, 768)\n",
            "156 Element has beed embedded with shape (17, 768)\n",
            "157 Element has beed embedded with shape (16, 768)\n",
            "158 Element has beed embedded with shape (18, 768)\n",
            "159 Element has beed embedded with shape (35, 768)\n",
            "160 Element has beed embedded with shape (5, 768)\n",
            "161 Element has beed embedded with shape (7, 768)\n",
            "162 Element has beed embedded with shape (7, 768)\n",
            "163 Element has beed embedded with shape (20, 768)\n",
            "164 Element has beed embedded with shape (14, 768)\n",
            "165 Element has beed embedded with shape (6, 768)\n",
            "166 Element has beed embedded with shape (11, 768)\n",
            "167 Element has beed embedded with shape (9, 768)\n",
            "168 Element has beed embedded with shape (8, 768)\n",
            "169 Element has beed embedded with shape (31, 768)\n",
            "170 Element has beed embedded with shape (13, 768)\n",
            "171 Element has beed embedded with shape (6, 768)\n",
            "172 Element has beed embedded with shape (13, 768)\n",
            "173 Element has beed embedded with shape (7, 768)\n",
            "174 Element has beed embedded with shape (6, 768)\n",
            "175 Element has beed embedded with shape (5, 768)\n",
            "176 Element has beed embedded with shape (45, 768)\n",
            "177 Element has beed embedded with shape (16, 768)\n",
            "178 Element has beed embedded with shape (15, 768)\n",
            "179 Element has beed embedded with shape (57, 768)\n",
            "180 Element has beed embedded with shape (34, 768)\n",
            "181 Element has beed embedded with shape (15, 768)\n",
            "182 Element has beed embedded with shape (11, 768)\n",
            "183 Element has beed embedded with shape (17, 768)\n",
            "184 Element has beed embedded with shape (24, 768)\n",
            "185 Element has beed embedded with shape (10, 768)\n",
            "186 Element has beed embedded with shape (27, 768)\n",
            "187 Element has beed embedded with shape (5, 768)\n",
            "188 Element has beed embedded with shape (9, 768)\n",
            "189 Element has beed embedded with shape (7, 768)\n",
            "190 Element has beed embedded with shape (29, 768)\n",
            "191 Element has beed embedded with shape (30, 768)\n",
            "192 Element has beed embedded with shape (19, 768)\n",
            "193 Element has beed embedded with shape (10, 768)\n",
            "194 Element has beed embedded with shape (17, 768)\n",
            "195 Element has beed embedded with shape (20, 768)\n",
            "196 Element has beed embedded with shape (15, 768)\n",
            "197 Element has beed embedded with shape (17, 768)\n",
            "198 Element has beed embedded with shape (18, 768)\n",
            "199 Element has beed embedded with shape (16, 768)\n",
            "200 Element has beed embedded with shape (21, 768)\n",
            "201 Element has beed embedded with shape (27, 768)\n",
            "202 Element has beed embedded with shape (13, 768)\n",
            "203 Element has beed embedded with shape (14, 768)\n",
            "204 Element has beed embedded with shape (30, 768)\n",
            "205 Element has beed embedded with shape (38, 768)\n",
            "206 Element has beed embedded with shape (24, 768)\n",
            "207 Element has beed embedded with shape (28, 768)\n",
            "208 Element has beed embedded with shape (5, 768)\n",
            "209 Element has beed embedded with shape (5, 768)\n",
            "210 Element has beed embedded with shape (8, 768)\n",
            "211 Element has beed embedded with shape (6, 768)\n",
            "212 Element has beed embedded with shape (7, 768)\n",
            "213 Element has beed embedded with shape (9, 768)\n",
            "214 Element has beed embedded with shape (10, 768)\n",
            "215 Element has beed embedded with shape (14, 768)\n",
            "216 Element has beed embedded with shape (7, 768)\n",
            "217 Element has beed embedded with shape (24, 768)\n",
            "218 Element has beed embedded with shape (9, 768)\n",
            "219 Element has beed embedded with shape (8, 768)\n",
            "220 Element has beed embedded with shape (8, 768)\n",
            "221 Element has beed embedded with shape (8, 768)\n",
            "222 Element has beed embedded with shape (11, 768)\n",
            "223 Element has beed embedded with shape (7, 768)\n",
            "224 Element has beed embedded with shape (11, 768)\n",
            "225 Element has beed embedded with shape (8, 768)\n",
            "226 Element has beed embedded with shape (7, 768)\n",
            "227 Element has beed embedded with shape (8, 768)\n",
            "228 Element has beed embedded with shape (7, 768)\n",
            "229 Element has beed embedded with shape (8, 768)\n",
            "230 Element has beed embedded with shape (8, 768)\n",
            "231 Element has beed embedded with shape (11, 768)\n",
            "232 Element has beed embedded with shape (9, 768)\n",
            "233 Element has beed embedded with shape (10, 768)\n",
            "234 Element has beed embedded with shape (7, 768)\n",
            "235 Element has beed embedded with shape (7, 768)\n",
            "236 Element has beed embedded with shape (7, 768)\n",
            "237 Element has beed embedded with shape (8, 768)\n",
            "238 Element has beed embedded with shape (7, 768)\n",
            "239 Element has beed embedded with shape (8, 768)\n",
            "240 Element has beed embedded with shape (9, 768)\n",
            "241 Element has beed embedded with shape (10, 768)\n",
            "242 Element has beed embedded with shape (10, 768)\n",
            "243 Element has beed embedded with shape (7, 768)\n",
            "244 Element has beed embedded with shape (8, 768)\n",
            "245 Element has beed embedded with shape (9, 768)\n",
            "246 Element has beed embedded with shape (10, 768)\n",
            "247 Element has beed embedded with shape (8, 768)\n",
            "248 Element has beed embedded with shape (9, 768)\n",
            "249 Element has beed embedded with shape (8, 768)\n",
            "250 Element has beed embedded with shape (8, 768)\n",
            "251 Element has beed embedded with shape (9, 768)\n",
            "252 Element has beed embedded with shape (9, 768)\n",
            "253 Element has beed embedded with shape (8, 768)\n",
            "254 Element has beed embedded with shape (8, 768)\n",
            "255 Element has beed embedded with shape (8, 768)\n",
            "256 Element has beed embedded with shape (8, 768)\n",
            "257 Element has beed embedded with shape (9, 768)\n",
            "258 Element has beed embedded with shape (11, 768)\n",
            "259 Element has beed embedded with shape (10, 768)\n",
            "260 Element has beed embedded with shape (10, 768)\n",
            "261 Element has beed embedded with shape (11, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(\"text\", axis=1)\n",
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odWG93rfRUuZ",
        "outputId": "9a040bb2-00f7-423a-d35c-ef87af74643e"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['label', 'embedding'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelDataset(Dataset):\n",
        "  def __init__(self, df, transformation=None):\n",
        "    self.df = df.copy()\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    label, embedding = self.df.iloc[idx].values # first return is the label, it's conformed don't change it\n",
        "    # embedding = embedding.numpy()  # I don't need this\n",
        "    embedding = embedding\n",
        "    embedding = torch.from_numpy(embedding)\n",
        "    label = torch.tensor(label)\n",
        "    return embedding, label"
      ],
      "metadata": {
        "id": "cZzu2GjVGT7u"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    sequences = [item[0] for item in batch]\n",
        "    labels = torch.tensor([item[1] for item in batch], dtype=torch.float)\n",
        "    lengths = torch.tensor([seq.size(0) for seq in sequences], dtype=torch.long)\n",
        "\n",
        "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
        "    return padded_sequences, labels, lengths\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "training_dataset = ModelDataset(train_df)\n",
        "testing_dataset = ModelDataset(test_df)\n",
        "\n",
        "train_dataloader = DataLoader(training_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "test_dataloader = DataLoader(testing_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "DQ6tgADPIv65"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.lstm = nn.LSTM(input_size = 768 , hidden_size=128, num_layers=3, dropout=0.2, batch_first=True)\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "    self.fc = nn.Linear(128,32)\n",
        "    self.output = nn.Linear(32, 1)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x, lengths=None):\n",
        "    if lengths is not None:\n",
        "      x = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "    output, (h_n, c_n) = self.lstm(x)\n",
        "    # output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
        "    last_hidden = h_n[-1]\n",
        "    x = self.fc(last_hidden)\n",
        "    x = self.relu(x)\n",
        "    x = self.dropout(x)\n",
        "    return self.output(x)\n"
      ],
      "metadata": {
        "id": "vjTFyKnrQMZI"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "03KtyjmGQA2l",
        "outputId": "b4e3b1de-068f-4c99-9d45-618a3d79e982"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"label\"].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "ZNYH2MAQUnjW",
        "outputId": "8f7d5424-fc3d-475c-e10d-572f5376a9e8"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "0    193\n",
              "1     69\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>69</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "model = Model()\n",
        "model = model.to(device)\n",
        "\n",
        "num_pos = 69\n",
        "num_neg = 193\n",
        "\n",
        "pos_weight = torch.tensor(num_neg / num_pos).to(device)\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "opt = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n",
        "\n",
        "epochs = 8\n",
        "\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "test_losses = []\n",
        "test_accs = []\n",
        "best_model = 0\n",
        "best_acc = 0\n",
        "for epoch in range(epochs):\n",
        "    # --- TRAINING ---\n",
        "    model.train()\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    train_loss = 0\n",
        "\n",
        "    for x, y, lengths in train_dataloader:\n",
        "        x = x.float().to(device)\n",
        "        y = y.float().to(device).view(-1)\n",
        "        lengths = lengths.to(device)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        y_hat = model(x, lengths).squeeze(-1).view(-1)\n",
        "        loss = loss_fn(y_hat, y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        y_pred = (torch.sigmoid(y_hat) >= 0.5).float()\n",
        "        train_correct += (y_pred == y).sum().item()\n",
        "        train_total += y.size(0)\n",
        "\n",
        "    train_acc = train_correct / train_total\n",
        "    train_loss /= len(train_dataloader)\n",
        "\n",
        "    # --- TESTING ---\n",
        "    model.eval()\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    test_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y, lengths in test_dataloader:\n",
        "            x = x.float().to(device)\n",
        "            y = y.float().to(device).view(-1)\n",
        "            lengths = lengths.to(device)\n",
        "\n",
        "            y_hat = model(x, lengths).squeeze(-1).view(-1)\n",
        "            loss = loss_fn(y_hat, y)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            y_pred = (torch.sigmoid(y_hat) >= 0.5).float()\n",
        "            test_correct += (y_pred == y).sum().item()\n",
        "            test_total += y.size(0)\n",
        "\n",
        "    test_acc = test_correct / test_total\n",
        "    test_loss /= len(test_dataloader)\n",
        "\n",
        "    if test_acc > best_acc:\n",
        "      best_acc = test_acc\n",
        "      best_model = copy.deepcopy(model)\n",
        "\n",
        "      print(\"Saved best model\")\n",
        "\n",
        "    # --- LOGGING ---\n",
        "    print(f\"Epoch {epoch + 1} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}\")\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accs.append(test_acc)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BczTyh73UK5j",
        "outputId": "4a170d1f-8bf7-4be4-a610-613426237ecb"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved best model\n",
            "Epoch 1 | Train Loss: 1.0032 | Train Acc: 0.2584 | Test Loss: 0.9916 | Test Acc: 0.2830\n",
            "Epoch 2 | Train Loss: 0.9908 | Train Acc: 0.2584 | Test Loss: 1.0076 | Test Acc: 0.2830\n",
            "Epoch 3 | Train Loss: 0.9739 | Train Acc: 0.2632 | Test Loss: 0.9473 | Test Acc: 0.2830\n",
            "Saved best model\n",
            "Epoch 4 | Train Loss: 0.9277 | Train Acc: 0.5072 | Test Loss: 0.9378 | Test Acc: 0.8491\n",
            "Saved best model\n",
            "Epoch 5 | Train Loss: 0.8233 | Train Acc: 0.9091 | Test Loss: 0.7633 | Test Acc: 0.9811\n",
            "Saved best model\n",
            "Epoch 6 | Train Loss: 0.6381 | Train Acc: 0.9713 | Test Loss: 0.5318 | Test Acc: 1.0000\n",
            "Epoch 7 | Train Loss: 0.4638 | Train Acc: 0.9904 | Test Loss: 0.3385 | Test Acc: 1.0000\n",
            "Epoch 8 | Train Loss: 0.3161 | Train Acc: 1.0000 | Test Loss: 0.2232 | Test Acc: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"model4.pth\")"
      ],
      "metadata": {
        "id": "I0awq4dnRMyC"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text, threshold=0.75):\n",
        "  tokens = tokenizer(text, return_tensors=\"pt\")\n",
        "  embedding = embedder(**tokens) # equivilant to embedder(tokens['input_ids'],tokens['attention_mask'])\n",
        "  embedding = embedding[0][0].detach().cpu()\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    y_hat = model(embedding)\n",
        "    y_hat = torch.sigmoid(y_hat).item()\n",
        "\n",
        "  result = (threshold <= y_hat)\n",
        "  return f\"{result} : {y_hat:4f}\"\n",
        "\n"
      ],
      "metadata": {
        "id": "1sViodoATwB7"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(\n",
        "\"\"\"\n",
        "تسوي\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "TSAHKoIxUq5X",
        "outputId": "10896391-4576-42c5-e37f-437fb7b7d04f"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'False : 0.382781'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positive_test_texts = [\n",
        "    \"تسوي أعذار طبية مضمونه\",\n",
        "    \"حل واجبات ومشاريع دراسية بجودة عالية\",\n",
        "    \"تقديم تقارير وأبحاث متكاملة لجميع التخصصات\",\n",
        "    \"خدمات طلابية متكاملة في مشاريع التخرج\",\n",
        "    \"عمل سكليفات طبية معتمدة من صحتي\",\n",
        "    \"حل جميع الواجبات والاختبارات بوقت قياسي\",\n",
        "    \"إعداد بحوث ورسائل ماجستير ودكتوراه\",\n",
        "    \"تصميم عروض بوربوينت وتقارير دراسية\",\n",
        "    \"كتابة وتلخيص المقالات العلمية\",\n",
        "    \"إعداد تكاليف ومشاريع دراسية مضمونة\",\n",
        "    \"خدمة متميزة لحل المشكلات الأكاديمية\",\n",
        "    \"تسليم مشاريع التخرج بجودة احترافية\",\n",
        "    \"تحضير عروض تقديمية مميزة لجميع المواد\",\n",
        "    \"كتابة الأبحاث العلمية باحترافية عالية\",\n",
        "    \"تقديم المساعدة في الواجبات المنزلية\",\n",
        "    \"حل الأسايمنت والاختبارات مع ضمان الدرجة\",\n",
        "    \"تصميم ملخصات دراسية سهلة الفهم\",\n",
        "    \"دعم في كتابة المقالات والبحوث\",\n",
        "    \"إعداد مشاريع التخرج في جميع التخصصات\",\n",
        "    \"توفير سكليفات طبية رسمية ومضمونة\"\n",
        "]\n",
        "\n",
        "negative_test_texts = [\n",
        "    \"مشاريعنا واجد ومربكة\",\n",
        "    \"ضغط مشاريع ما يخلص\",\n",
        "    \"المشاريع ذبحتنا والله\",\n",
        "    \"أحد عنده حل الواجب؟\",\n",
        "    \"المشروع تعجيزي بشكل كبير\",\n",
        "    \"مشاريع صعبة وما نفهمها\",\n",
        "    \"ما عندي وقت أحل الواجبات\",\n",
        "    \"مشاريع الدراسة تأخذ كل وقتي\",\n",
        "    \"وين ألقى حل للواجب؟\",\n",
        "    \"مشاريع معقدة ومحبطة\",\n",
        "    \"مشاكل في فهم المطلوب\",\n",
        "    \"تعبت من كثرة المشاريع والواجبات\",\n",
        "    \"ما أدري شلون أخلص المشروع\",\n",
        "    \"مشاريع متعبة ومحبطة\",\n",
        "    \"مشاريع صعبة جدا وما في حلول\",\n",
        "    \"ما فهمت المطلوب في المشروع\",\n",
        "    \"مشاريع معقدة وتحتاج مساعدة\",\n",
        "    \"ما ألقى حد يساعدني في المشروع\",\n",
        "    \"المشروع غير واضح ومبهم\",\n",
        "    \"مشاريع مرهقة ومحبطة جداً\"\n",
        "]\n",
        "\n",
        "for t in positive_test_texts:\n",
        "  print(predict(t))\n",
        "\n",
        "print()\n",
        "print()\n",
        "\n",
        "for t in negative_test_texts:\n",
        "  print(predict(t))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAjeB0EWU90d",
        "outputId": "60fd14ed-f4d1-4507-8f23-d03804a15285"
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True : 0.832078\n",
            "True : 0.847335\n",
            "True : 0.845804\n",
            "True : 0.845601\n",
            "True : 0.856987\n",
            "True : 0.853052\n",
            "True : 0.844613\n",
            "True : 0.850214\n",
            "True : 0.838108\n",
            "True : 0.836252\n",
            "True : 0.835173\n",
            "True : 0.840805\n",
            "True : 0.851641\n",
            "True : 0.847101\n",
            "True : 0.791543\n",
            "True : 0.856762\n",
            "True : 0.847784\n",
            "True : 0.826004\n",
            "True : 0.848102\n",
            "True : 0.853967\n",
            "\n",
            "\n",
            "False : 0.235946\n",
            "False : 0.272286\n",
            "False : 0.244860\n",
            "False : 0.196010\n",
            "False : 0.346791\n",
            "False : 0.467127\n",
            "False : 0.144665\n",
            "False : 0.262793\n",
            "False : 0.129117\n",
            "False : 0.683481\n",
            "False : 0.229034\n",
            "False : 0.184679\n",
            "False : 0.131185\n",
            "False : 0.650024\n",
            "False : 0.225384\n",
            "False : 0.142873\n",
            "False : 0.646466\n",
            "False : 0.130977\n",
            "False : 0.247643\n",
            "False : 0.260327\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "6KkYCb_xfswl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}